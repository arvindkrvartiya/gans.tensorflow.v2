{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN with MNIST\n",
    "\n",
    "* MNIST data를 가지고 **Generative Adversarial Networks**를 `tf.contrib.slim`을 이용하여 만들어보자.\n",
    "  * [참고: TensorFlow slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "tf.set_random_seed(219)\n",
    "np.random.seed(219)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = 'train/gan/exp1/'\n",
    "max_epochs = 100\n",
    "save_epochs = 20\n",
    "summary_steps = 5000\n",
    "print_steps = 2000\n",
    "batch_size = 32\n",
    "learning_rate_D = 0.0002\n",
    "learning_rate_G = 0.0002\n",
    "k = 1 # the number of step of learning D before learning G\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), _ = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.reshape([-1, 28*28])\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=max_epochs)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_model(object):\n",
    "  \"\"\"Generative Adversarial Networks\n",
    "  implementation based on http://arxiv.org/abs/1406.2661\n",
    "  \n",
    "  \"Generative Adversarial Nets\"\n",
    "  Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\n",
    "  David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, mode, train_dataset, test_dataset=None):\n",
    "    \"\"\"Basic setup.\n",
    "    \n",
    "    Args:\n",
    "      mode: \"train\" or \"generate\"\n",
    "    \"\"\"\n",
    "    assert mode in [\"train\", \"generate\"]\n",
    "    self.mode = mode\n",
    "    \n",
    "    # hyper-parameters for model\n",
    "    self.x_dim = 784\n",
    "    self.z_dim = 100\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = num_samples\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    \n",
    "    # Global step Tensor.\n",
    "    self.global_step = None\n",
    "    \n",
    "    print('The mode is %s.' % self.mode)\n",
    "    print('complete initializing model.')\n",
    "    \n",
    "    \n",
    "  def build_random_z_inputs(self):\n",
    "    \"\"\"Build a vector random_z in latent space.\n",
    "    \n",
    "    Returns:\n",
    "      self.random_z (2-rank Tensor with [batch_size, z_dim]): \n",
    "          latent vector which size is generally 100 dim.\n",
    "    \"\"\"\n",
    "    # Setup variable of random vector z\n",
    "    with tf.variable_scope('random_z'):\n",
    "      #self.random_z = tf.placeholder(tf.float32, [None, self.z_dim])\n",
    "      self.random_z = tf.random_normal([self.batch_size, self.z_dim])\n",
    "      self.sample_random_z = tf.random_normal([self.num_samples, self.z_dim])\n",
    "\n",
    "    return self.random_z, self.sample_random_z\n",
    "  \n",
    "  \n",
    "  def read_MNIST(self, dataset):\n",
    "    \"\"\"Read MNIST dataset\n",
    "    \n",
    "    Args:\n",
    "      dataset (`tf.data.Dataset` format): MNIST dataset\n",
    "      \n",
    "    Returns:\n",
    "      self.mnist (2-rank Tensor with [batch, 28 * 28]): MNIST dataset with batch size\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('mnist'):\n",
    "      iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "      self.mnist = iterator.get_next()\n",
    "      self.mnist = tf.cast(self.mnist, dtype = tf.float32)\n",
    "      \n",
    "    return self.mnist\n",
    "\n",
    "\n",
    "  def Generator(self, random_z, reuse=False):\n",
    "    \"\"\"Generator setup.\n",
    "    \n",
    "    Args:\n",
    "      random_z: A float32 Tensor random vector (latent code)\n",
    "      reuse: variable reuse flag\n",
    "      \n",
    "    Returns:\n",
    "      A float32 scalar Tensor of generated images from random vector\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "      with slim.arg_scope([slim.fully_connected],\n",
    "                          activation_fn=tf.nn.leaky_relu):\n",
    "        # inputs = random_z: 100 dim\n",
    "        self.layer1 = slim.fully_connected(inputs=random_z,\n",
    "                                           num_outputs=256,\n",
    "                                           scope='layer1')\n",
    "        # layer1: 256 dim\n",
    "        self.layer2 = slim.fully_connected(inputs=self.layer1,\n",
    "                                           num_outputs=512,\n",
    "                                           scope='layer2')\n",
    "        # layer2: 512 dim\n",
    "        self.layer3 = slim.fully_connected(inputs=self.layer2,\n",
    "                                           num_outputs=1024,\n",
    "                                           scope='layer3')\n",
    "        # layer3: 1024 dim\n",
    "        self.layer4 = slim.fully_connected(inputs=self.layer3,\n",
    "                                           num_outputs=self.x_dim,\n",
    "                                           activation_fn=tf.sigmoid,\n",
    "                                           scope='layer4')\n",
    "        # output = layer4: x_dim (784)\n",
    "        generated_data = self.layer4\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "  def Discriminator(self, data, reuse=False):\n",
    "    \"\"\"Discriminator setup.\n",
    "    \n",
    "    Args:\n",
    "      data: A float32 scalar Tensor of real data\n",
    "      reuse: reuse flag\n",
    "      \n",
    "    Returns:\n",
    "      logits: A float32 scalar Tensor\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "      with slim.arg_scope([slim.fully_connected],\n",
    "                          activation_fn=tf.nn.leaky_relu):\n",
    "        with slim.arg_scope([slim.dropout], keep_prob=0.7):\n",
    "          # data: x_dim (784)\n",
    "          self.layer1 = slim.fully_connected(inputs=data,\n",
    "                                             num_outputs=1024,\n",
    "                                             scope='layer1')\n",
    "          self.layer1_drop = slim.dropout(inputs=self.layer1, scope='drop1')\n",
    "          # layer1: 1024 dim\n",
    "          self.layer2 = slim.fully_connected(inputs=self.layer1_drop,\n",
    "                                             num_outputs=512,\n",
    "                                             scope='layer2')\n",
    "          self.layer2_drop = slim.dropout(inputs=self.layer2, scope='drop2')\n",
    "          # layer2: 512 dim\n",
    "          self.layer3 = slim.fully_connected(inputs=self.layer2_drop,\n",
    "                                             num_outputs=256,\n",
    "                                             scope='layer3')\n",
    "          self.layer3_drop = slim.dropout(inputs=self.layer3, scope='drop3')\n",
    "          # layer3: 256 dim\n",
    "          self.layer4 = slim.fully_connected(inputs=self.layer3_drop,\n",
    "                                             num_outputs=1,\n",
    "                                             activation_fn=None,\n",
    "                                             scope='layer4')\n",
    "          # logits = layer4: 1 dim\n",
    "          discriminator_logits = self.layer4\n",
    "\n",
    "          return discriminator_logits\n",
    "    \n",
    "    \n",
    "  def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      #self.global_step = tf.Variable(initial_value=0,\n",
    "      #                               name='global_step',\n",
    "      #                               trainable=False,\n",
    "      #                               collections=[tf.GraphKeys.GLOBAL_STEP,\n",
    "      #                                            tf.GraphKeys.GLOBAL_VARIABLES])\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "  def GANLoss(self, logits, is_real=True, scope=None):\n",
    "    \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "      logits: A float32 Tensor of logits.\n",
    "      is_real: boolean, Treu means `1` labeling, False means `0` labeling.\n",
    "      \n",
    "    Returns:\n",
    "      A scalar Tensor representing the loss value.\n",
    "    \"\"\"\n",
    "    if is_real:\n",
    "      labels = tf.ones_like(logits)\n",
    "    else:\n",
    "      labels = tf.zeros_like(logits)\n",
    "\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                           logits=logits,\n",
    "                                           scope=scope)\n",
    "\n",
    "    return loss\n",
    "\n",
    "      \n",
    "  def build(self):\n",
    "    \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "    self.setup_global_step()\n",
    "    \n",
    "    if self.mode == \"generate\":\n",
    "      pass\n",
    "    \n",
    "    else:\n",
    "      # generating random vector\n",
    "      self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z)\n",
    "      \n",
    "      # read dataset\n",
    "      self.real_data = self.read_MNIST(self.train_dataset)\n",
    "      \n",
    "      # discriminating real data by Discriminator()\n",
    "      self.real_logits = self.Discriminator(self.real_data)\n",
    "      # discriminating fake data (generated)_images) by Discriminator()\n",
    "      self.fake_logits = self.Discriminator(self.generated_data, reuse=True)\n",
    "      \n",
    "      # losses of real with label \"1\"\n",
    "      self.loss_real = self.GANLoss(logits=self.real_logits, is_real=True, scope='loss_D_real')\n",
    "      # losses of fake with label \"0\"\n",
    "      self.loss_fake = self.GANLoss(logits=self.fake_logits, is_real=False, scope='loss_D_fake')\n",
    "      \n",
    "      # losses of Discriminator\n",
    "      with tf.variable_scope('loss_D'):\n",
    "        self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "      # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "      self.loss_Generator = self.GANLoss(logits=self.fake_logits, is_real=True, scope='loss_G')\n",
    "      \n",
    "      # Separate variables for each function\n",
    "      self.D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "      self.G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "      \n",
    "      \n",
    "      # generating images for sample\n",
    "      self.sample_data = self.Generator(self.sample_random_z, reuse=True)\n",
    "      \n",
    "      # write summaries\n",
    "      # Add loss summaries\n",
    "      tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "      tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "      \n",
    "      # Add histogram summaries\n",
    "      for var in self.D_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      for var in self.G_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      \n",
    "      # Add image summaries\n",
    "      tf.summary.image('random_images', tf.reshape(self.generated_data, [-1, 28, 28, 1]), max_outputs=4)\n",
    "      tf.summary.image('real_images', tf.reshape(self.real_data, [-1, 28, 28, 1]))\n",
    "      \n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_data(sample_data, max_print=num_samples):\n",
    "  print_images = sample_data[:max_print,:]\n",
    "  print_images = print_images.reshape([max_print, 28, 28])\n",
    "  print_images = print_images.swapaxes(0, 1)\n",
    "  print_images = print_images.reshape([28, max_print * 28])\n",
    "  \n",
    "  plt.figure(figsize=(max_print, 1))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_images, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN_model(mode=\"train\", train_dataset=train_dataset)\n",
    "model.build()\n",
    "\n",
    "# show info for trainable variables\n",
    "t_vars = tf.trainable_variables()\n",
    "slim.model_analyzer.analyze_vars(t_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D, beta1=0.5)\n",
    "opt_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_D_op = opt_D.minimize(model.loss_Discriminator, var_list=model.D_vars)\n",
    "opt_G_op = opt_G.minimize(model.loss_Generator, global_step=model.global_step,\n",
    "                          var_list=model.G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_location = train_dir\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.train.Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  tf.logging.info('Start Session.')\n",
    "  \n",
    "  num_examples = len(train_data)\n",
    "  num_batches_per_epoch = int(num_examples / batch_size)\n",
    "  \n",
    "  # save loss values for plot\n",
    "  loss_history = []\n",
    "  pre_epochs = 0\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      for _ in range(k):\n",
    "        _, loss_D = sess.run([opt_D_op, model.loss_Discriminator])\n",
    "      _, global_step_, loss_G = sess.run([opt_G_op,\n",
    "                                          model.global_step,\n",
    "                                          model.loss_Generator])\n",
    "      \n",
    "      epochs = global_step_ * batch_size / float(num_examples)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if global_step_ % print_steps == 0:\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3f} loss_G: {:.3f} ({:.2f} examples/sec; {:.2f} sec/batch)\".format(\n",
    "                  epochs, global_step_, loss_D, loss_G, examples_per_sec, duration))\n",
    "\n",
    "        loss_history.append([epochs, loss_D, loss_G])\n",
    "\n",
    "        # print sample data\n",
    "        sample_data = sess.run(model.sample_data)\n",
    "        print_sample_data(sample_data)\n",
    "\n",
    "      # write summaries periodically\n",
    "      if global_step_ % summary_steps == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        train_writer.add_summary(summary_str, global_step=global_step_)\n",
    "\n",
    "      # save model checkpoint periodically\n",
    "      if int(epochs) % save_epochs == 0  and  pre_epochs != int(epochs):\n",
    "        tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "        saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "        pre_epochs = int(epochs)\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "      saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "      break\n",
    "      \n",
    "  tf.logging.info('complete training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "\n",
    "plt.plot(loss_history[:,0], loss_history[:,1], label='loss_D')\n",
    "plt.plot(loss_history[:,0], loss_history[:,2], label='loss_G')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
